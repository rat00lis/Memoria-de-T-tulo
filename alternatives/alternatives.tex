\section{Evaluación de Alternativas}
\label{alternatives}

\subsection{Técnicas de Downsampling}

Una vez introducido el concepto de downsampling como solución al problema de visualización de series de tiempo muy densas, es necesario revisar y comparar las principales alternativas existentes.

Diversos autores han propuesto algoritmos que permiten seleccionar los puntos más representativos de una serie para facilitar su visualización. A continuación, se presenta un resumen de los métodos más relevantes:

\begin{enumerate}
    \item \textbf{Mode-Median-Bucket (MMB)}: 
    Divide los datos en bloques y selecciona un punto por bloque usando la moda o la mediana de los valores. Es simple y fácil de entender, pero tiende a omitir picos y valles locales relevantes~\cite{steinarsson2013downsampling}.

    \item \textbf{Min-Std-Error-Bucket (MSEB)}: 
    Utiliza regresión lineal para calcular el error estándar entre pares de puntos, eligiendo aquellos que minimizan la suma de errores. Produce resultados estadísticamente coherentes, pero suaviza demasiado la serie y elimina detalles visuales importantes~\cite{steinarsson2013downsampling}.

    \item \textbf{Longest-Line-Bucket (LLB)}: 
    Similar a MSEB, pero en lugar de minimizar el error, maximiza la longitud total de las líneas entre puntos seleccionados. Tiene mejor capacidad para conservar picos extremos y fluctuaciones relevantes~\cite{steinarsson2013downsampling}.

    \item \textbf{Largest-Triangle-Three-Buckets (LTTB)}: 
    Divide los datos en tres bloques consecutivos y selecciona el punto que forma el triángulo de mayor área con los puntos de los bloques adyacentes. Esta técnica preserva bien la forma general del gráfico y es eficiente computacionalmente~\cite{steinarsson2013downsampling}.

    \item \textbf{Largest-Triangle-Dynamic (LTD)}: 
    Variante del LTTB que adapta dinámicamente el tamaño de los bloques según la variación local de los datos. Mejora la representación visual en series con regiones muy fluctuantes y otras más estables~\cite{steinarsson2013downsampling}.

    \item \textbf{MinMaxLTTB}: 
    Propuesto por Van Der Donckt {\it et al.}~\cite{vanderdonckt2023minmaxlttb}, este algoritmo mejora la escalabilidad de LTTB al aplicar una preselección eficiente de puntos mínimos y máximos verticales mediante el algoritmo MinMax, para luego aplicar LTTB solo sobre esos puntos seleccionados. De esta manera, reduce significativamente el tiempo de cómputo sin sacrificar calidad visual.
\end{enumerate}

Dado el enfoque de este trabajo, se seleccionó principalmente el uso del algoritmo \textbf{MinMaxLTTB}. Esta decisión se fundamenta en varios factores: en primer lugar, el algoritmo ha sido propuesto por los mismos autores que desarrollaron la biblioteca \texttt{tsdownsample}, lo que garantiza una implementación de referencia optimizada y bien documentada. Además, MinMaxLTTB presenta un excelente equilibrio entre rendimiento y fidelidad visual, al combinar una reducción sustancial en el tiempo de cómputo con una buena preservación de la forma general de la serie de tiempo.

Como complemento para el análisis comparativo, también se utilizará el algoritmo \textbf{LTTB}, ya que es ampliamente citado en la literatura como una opción base eficiente, y su comparación directa con MinMaxLTTB permite observar el impacto de la estrategia de preselección aplicada por los autores.

\subsection{Estructuras de Datos Compactas}

El uso de estructuras de datos compactas permite representar secuencias numéricas en memoria utilizando una menor cantidad de bits por elemento, manteniendo el acceso a los datos sin pérdida de información. 

En la literatura existen bibliotecas que implementan estas estructuras de datos compactas. En particular, utilizaremos la biblioteca \texttt{sdsl4py}, un conjunto de bindings en Python para la Succinct Data Structure Library (SDSL), que permite acceder a múltiples representaciones compactas desde  Python. A continuación se presentan las  estructuras de datos compactas que fueron consideradas en esta memoria de título:

\begin{enumerate}
    \item \textbf{enc\_vector\_elias\_gamma}:
    Utiliza codificación de enteros Elias-Gamma. Es eficiente para valores pequeños, ya que su tamaño codificado crece logarítmicamente con el valor. No puede representar ceros.

    \item \textbf{enc\_vector\_elias\_delta}:
    Variante de Elias-Gamma que mejora la compresión para valores grandes, manteniendo eficiencia en lectura. También requiere que todos los valores sean mayores que cero.

    \item \textbf{enc\_vector\_fibonacci}:
    Usa codificación basada en la sucesión de Fibonacci. Esta técnica es compacta y libre de prefijos, pero tiene limitaciones para valores cero y una complejidad levemente mayor en decodificación.

    \item \textbf{enc\_vector\_comma\_2}:
    Codificación por comas binarias (comma-2), una forma simple y eficiente de codificar enteros con separación de bits mediante una secuencia especial. Ofrece buena compresión para secuencias no muy dispersas.

    \item \textbf{vlc\_vector\_elias\_delta}:
    Variante del enc\_vector\_elias\_delta que permite acceso más rápido a los elementos gracias a una estructura de lectura optimizada (Variable-Length Codes con índice de acceso directo).

    \item \textbf{vlc\_vector\_elias\_gamma}:
    Equivalente a la anterior, pero usando codificación Elias-Gamma. Útil cuando se prioriza el acceso sobre la compresión máxima.

    \item \textbf{vlc\_vector\_fibonacci}:
    Combina las ventajas de codificación Fibonacci con accesos más eficientes mediante índices auxiliares.

    \item \textbf{vlc\_vector\_comma\_2}:
    Aplica codificación comma-2 con estructura de acceso rápido. Es útil cuando se necesita acceder a datos comprimidos con latencias bajas.

    \item \textbf{dac\_vector}:
    Utiliza Directly Addressable Codes (DAC), una estructura escalonada que permite un buen balance entre compresión y velocidad de acceso. Es especialmente útil para secuencias de enteros donde los valores tienen alta varianza.

\end{enumerate}

Para evaluar estas estructuras de datos, se han medido tres métricas clave: el tiempo de acceso a los elementos, el tiempo de construcción de la estructura y el espacio utilizado en memoria. 

\paragraph{Input}
\vspace{0.2cm}

Para garantizar una evaluación comprehensiva y representativa del comportamiento de las estructuras de datos comprimidas, se han diseñado cuatro tipos de vectores sintéticos que simulan diferentes patrones de datos comúnmente encontrados en aplicaciones reales:

\begin{enumerate}
    \item \textbf{Incremental pequeño (\texttt{incremental\_small}):} Vectores con valores crecientes que presentan pequeñas diferencias entre elementos consecutivos. Este patrón simula series temporales con crecimiento gradual o datos de sensores con variaciones mínimas. Los valores de $x$ siguen la secuencia $[1, 2, 3, ..., n]$, mientras que los valores de $y$ añaden una pequeña variación aleatoria en el rango $[0, 2]$ a cada posición.

    \item \textbf{Aleatorio (\texttt{random}):} Vectores con valores completamente aleatorios distribuidos uniformemente en un rango amplio. Este patrón representa el peor caso para algoritmos de compresión, ya que no presenta patrones predecibles. Tanto $x$ como $y$ contienen enteros aleatorios en el rango $[1, n \times 10]$.

    \item \textbf{Oscilante (\texttt{oscillating}):} Vectores que siguen un patrón de onda sinusoidal, alternando entre valores ascendentes y descendentes. Este comportamiento es típico en señales periódicas, datos de vibración o patrones cíclicos. Los valores de $x$ son secuenciales, mientras que $y$ sigue la función $y_i = \frac{n}{2}(1 + \sin(\frac{4\pi i}{n}))$.

    \item \textbf{Incremental grande (\texttt{incremental\_large}):} Vectores con valores crecientes que presentan grandes diferencias entre elementos consecutivos. Este patrón simula datos con crecimiento exponencial o escalas logarítmicas. Los valores de $x$ son secuenciales $[1, 2, 3, ..., n]$, y los valores de $y$ se calculan como $y_i = i \times \text{random}[50, 200]$.
\end{enumerate}

Cada vector de prueba contiene por defecto 10,000 elementos, proporcionando un volumen de datos suficiente para evaluar el rendimiento de las estructuras de datos compactas. Esta diversidad de patrones permite analizar cómo cada método de compresión se adapta a diferentes características de los datos, desde el caso ideal (datos con alta correlación) hasta el caso más desafiante (datos completamente aleatorios).

\paragraph{Resultados}
\vspace{0.2cm}

A continuación, se presenta un promedio de los resultados obtenidos al evaluar las distintas estructuras de datos compactas utilizando los vectores sintéticos mencionados anteriormente.

% añadir imagenes y tablas de resultados
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{alternatives/scatter_plot/3d_scatter_plot_structures.png}
\caption{Puntajes promedio de las estructuras de datos compactas}   
\label{fig:scatter_plot_structures}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{alternatives/scatter_plot/euclidian_distance_from_ideal.png}
\caption{Distancia euclidiana promedio de las estructuras de datos compactas respecto al punto $(0, 0, 0)$}   
\label{fig:euclidian_distance_from_ideal}
\end{figure}

\input{alternatives/scatter_plot/latex_ranking_table.tex}

% Como podemos apreciar en el Anexo~\ref{anexo_sdsl4py}, al evaluar la velocidad de acceso, el tiempo de construcción y el espacio utilizado por las distintas alternativas de estructuras de datos compactas, no se observa una diferencia significativa en cuanto a su curva de complejidad teórica. No obstante, para determinar cuál estructura resulta más adecuada en la práctica, se define una \textbf{relación compuesta} que pondera las tres métricas principales.

% Esta relación se formula como:
% %scatter plot para seleccionar la estructura de datos compacta 3d

% \begin{equation}
% \text{Score}(E) = \alpha \cdot \text{AccessTime}(E) + \beta \cdot \text{BuildTime}(E) + \gamma \cdot \text{SpaceUsed}(E)
% \end{equation}

% donde:
% \begin{itemize}
%     \item $E$ representa una estructura evaluada,
%     \item $\text{AccessTime}(E)$, $\text{BuildTime}(E)$ y $\text{SpaceUsed}(E)$ corresponden a los valores empíricos medidos para dicha estructura,
%     \item y $\alpha$, $\beta$, y $\gamma$ son coeficientes de ponderación tales que $\alpha + \beta + \gamma = 1$.
% \end{itemize}

% Estos coeficientes pueden ser ajustados en función de los objetivos específicos del sistema o aplicación. Por ejemplo, en un entorno donde el rendimiento en lectura sea prioritario, se puede asignar un valor mayor a $\alpha$; en cambio, si se trabaja con recursos de almacenamiento limitados, se puede incrementar el peso de $\gamma$. 

% En esta ocasión, se consideran los valores:

% \begin{itemize}
%     \item $\alpha = 0.6$
% 	\item $\beta = 0.4$
% 	\item $\gamma = 0.3$
% \end{itemize}

% \begin{table}[H]
% \centering
% \caption{Puntaje promedio por estructura (menor es mejor)}
% \label{tab:score_promedio_estructuras}
% \begin{tabular}{l r}
% \toprule 
% \textbf{Estructura} & \textbf{Score promedio} \\
% \midrule
% dac\_vector - 8              & 65899.74 \\
% vlc\_vector\_fibonacci - 8   & 69255.21 \\
% vlc\_vector\_elias\_delta - 8 & 73931.74 \\
% vlc\_vector\_elias\_gamma - 8 & 78023.47 \\
% vlc\_vector\_comma\_2 - 8     & 80850.67 \\
% Original Data                & 657595.20 \\
% \bottomrule
% \end{tabular}
% \end{table}

A partir de la puntuación resultante, se seleccionan las estructuras de datos \textit{dac\_vector} y \textit{vlc\_vector\_fibonacci} como las más adecuadas para el sistema, ya que son las que obtuvieron los puntajes más favorables en la evaluación.