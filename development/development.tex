\section{Desarrollo}


\subsection{Uso de \texttt{sdsl4py} para la visualización de datos}

La biblioteca \texttt{sdsl4py} expone en Python diversas estructuras de datos compactas de la librería \texttt{sdsl-lite}, diseñada originalmente para representar secuencias de enteros sin signo de forma eficiente en términos de espacio. El flujo de trabajo básico consiste en construir un \texttt{int\_vector} modificable con un tamaño predefinido, cargar los datos en él, y luego aplicar una estrategia de compresión específica. Una vez comprimido, el vector pasa a ser de solo lectura.

Aunque estas estructuras pueden integrarse con bibliotecas como \texttt{plotly}, presentan una limitación clave: no admiten valores flotantes ni negativos de forma nativa. Esto se debe a que las técnicas de compresión empleadas —como las codificaciones de Elias o Fibonacci— asumen enteros positivos, donde patrones de repetición o crecimiento logarítmico pueden ser aprovechados para reducir el espacio.

Cuando se trabaja con series de tiempo u otros datos con decimales, la representación compacta se vuelve más compleja. Los valores flotantes tienden a carecer de regularidad en su representación binaria, dificultando la compresión efectiva. Además, representar signos o fracciones dentro de estructuras diseñadas exclusivamente para enteros sin signo requiere soluciones externas, como escalado previo o codificaciones personalizadas, que introducen sobrecarga adicional y limitan la interoperabilidad con los algoritmos ya existentes.

Por estas razones, aunque \texttt{sdsl4py} puede ser útil para visualizar datos discretos o transformados a enteros, no resulta una opción directa ni eficiente para representar valores reales sin una etapa de preprocesamiento.


\subsection{CompressedVector}

Para superar la limitación de que \texttt{sdsl4py} sólo permite almacenar enteros sin signo (\textit{uint}), se propone una estructura llamada \texttt{CompressedVector}, que habilita la representación de números reales (positivos, negativos y \texttt{NaN}) mediante la combinación de varias estructuras compactas para representar vectores de enteros,\texttt{int\_vector<>}. La estrategia consiste en descomponer cada número flotante en tres componentes separados:

\begin{enumerate}
    \item \textbf{Parte entera}: el valor absoluto truncado del número.
    \item \textbf{Parte decimal}: los decimales, escalados como enteros según la precisión deseada.
    \item \textbf{Indicador de signo}: codificación entera que distingue entre valores positivos, negativos y \texttt{NaN}.
\end{enumerate}

Dado un número real $x \in \mathbb{R}$ y una precisión de $d$ dígitos decimales, la transformación se define como:

\begin{align*}
    x &= \pm \left( \lfloor |x| \rfloor + \frac{\mathrm{decimales}(x)}{10^d} \right) \\
    \text{parte\_entera} &= \lfloor |x| \rfloor \\
    \text{parte\_decimal} &= \left\lfloor (|x| - \lfloor |x| \rfloor) \cdot 10^d \right\rfloor \\
    \text{signo} &=
        \begin{cases}
            1 & \text{si } x \geq 0 \\
            0 & \text{si } x < 0 \\
            2 & \text{si } x \text{ es } \texttt{NaN}
        \end{cases}
\end{align*}

Cada uno de estos componentes se almacena en un \texttt{int\_vector<>} distinto, lo que permite aplicar técnicas de compresión sobre los mismos.
\vspace{1em}
\noindent
Es posible recuperar la $i$-ésima entrada del vector original mediante la operación inversa:

\begin{align*}
    \text{si } \text{signo}[i] = 2, \quad &\Rightarrow x_i = \texttt{NaN} \\
    \text{sino: } \quad &x_i = (-1)^{1 - \text{signo}[i]} \cdot \left( \text{parte\_entera}[i] + \frac{\text{parte\_decimal}[i]}{10^d} \right)
\end{align*}

\noindent
Este procedimiento está implementado en el método \texttt{\_reconstruct\_float\_value} de la clase \texttt{CompressedVector}, y utiliza la biblioteca \texttt{Decimal} de Python para preservar la precisión durante las operaciones aritméticas.

\vspace{1em}
\noindent
Si bien esta solución requiere más espacio que una única estructura de datos compacta, permite una representación robusta y precisa de números reales, manteniendo las ventajas de compresión que ofrece \texttt{sdsl4py} para enteros.

Para facilitar su uso dentro del ecosistema Python y su interoperabilidad con bibliotecas estándar, \texttt{CompressedVector} implementa múltiples operaciones comunes mediante funciones mágicas (\textit{dunder methods}). En la siguiente tabla se resumen las principales funcionalidades disponibles.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3} % Aumenta la altura de las filas de forma consistente
\begin{tabular}{|p{3.5cm}|p{6cm}|p{5cm}|}
\hline
\textbf{Operación} & \textbf{Descripción} & \textbf{Método implementado} \\
\hline
\rule{0pt}{1.5em}Indexación         & Acceso a un elemento por índice                & \texttt{\_\_getitem\_\_(index)} \\
\hline
\rule{0pt}{1.5em}Asignación         & Modificación de un valor en un índice dado     & \texttt{\_\_setitem\_\_(index, value)} \\
\hline
\rule{0pt}{1.5em}Tamaño             & Cantidad de elementos almacenados              & \texttt{\_\_len\_\_()} \\
\hline
\rule{0pt}{1.5em}Iteración          & Permite iterar sobre los elementos             & \texttt{\_\_iter\_\_()} \\
\hline
\rule{0pt}{1.5em}Representación     & Muestra formato legible para impresión         & \texttt{\_\_repr\_\_()} \\
\hline
\rule{0pt}{1.5em}Suma               & Suma escalar a todos los elementos             & \texttt{\_\_add\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Suma in-place      & Suma escalar modificando el vector             & \texttt{\_\_iadd\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Resta              & Resta escalar a todos los elementos            & \texttt{\_\_sub\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Resta in-place     & Resta escalar modificando el vector            & \texttt{\_\_isub\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Multiplicación     & Multiplica todos los valores por escalar       & \texttt{\_\_mul\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Multiplicación in-place & Multiplica modificando el vector         & \texttt{\_\_imul\_\_(other)} \\
\hline
\rule{0pt}{1.5em}División           & Divide todos los elementos por un escalar      & \texttt{\_\_truediv\_\_(other)} \\
\hline
\rule{0pt}{1.5em}División in-place  & Divide modificando el vector                   & \texttt{\_\_itruediv\_\_(other)} \\
\hline
\rule{0pt}{1.5em}Comparación        & Compara igualdad elemento a elemento           & \texttt{\_\_eq\_\_(other)} \\
\hline
\end{tabular}
\caption{Operaciones implementadas en la clase \texttt{CompressedVector}}
\end{table}

\subsection{CompressedVectorDownsampler}

Hasta ahora hemos abordado por separado el uso de estructuras de datos compactas y las técnicas de \textit{downsampling}. La clase \texttt{CompressedVectorDownsampler} combina ambos enfoques en una única solución, permitiendo obtener datos listos para ser graficados tras haber sido comprimidos y reducidos.

Esta clase integra internamente las funcionalidades de \texttt{CompressedVector} y \texttt{TsDownsample}. El usuario puede proporcionar como entrada uno o ambos vectores originales (\texttt{x} e \texttt{y}), junto con los siguientes parámetros:

\begin{itemize}
    \item Cantidad de decimales a conservar.
    \item Tamaño del entero a utilizar (8, 16, 32 o 64 bits).
    \item Método de compresión (como función o identificador en forma de \texttt{string}).
    \item Estrategia de \textit{downsampling} (también como función o \texttt{string}).
    \item Número deseado de puntos de salida (\texttt{n\_out}).
\end{itemize}

Como resultado, se obtienen los vectores \texttt{x} y/o \texttt{y} ya procesados, tanto por \textit{downsampling} como por compresión, permitiendo al usuario almacenar información densa en una fracción del espacio original. Esto garantiza una reducción significativa en el uso de almacenamiento y en los tiempos de renderización durante la visualización.


\subsection{Librerías de Visualización}

El propósito de esta sección es explorar cómo las herramientas desarrolladas en este trabajo pueden integrarse con bibliotecas de visualización ya existentes. No se contempla la creación de una nueva biblioteca gráfica, sino la adaptación y evaluación de las opciones disponibles para visualizar datos procesados mediante compresión y \textit{downsampling}.

A continuación, se presentan las bibliotecas consideradas en este estudio, agrupadas según su compatibilidad con las estructuras propuestas.

\subsubsection{Bibliotecas Dependientes de NumPy}

Muchas bibliotecas populares de visualización en Python, como \texttt{matplotlib}~\cite{matplotlib}, \texttt{plotly} y \texttt{plotly-resampler}, ofrecen amplias funcionalidades para construir gráficos interactivos y estáticos. Sin embargo, todas ellas presentan una limitación importante: requieren explícitamente que los datos de entrada sean arreglos de tipo \texttt{numpy.ndarray}, o bien realizan una conversión interna hacia dicho formato.

Esto representa una incompatibilidad con \texttt{CompressedVector}, ya que \texttt{numpy} impone restricciones sobre el tipo y tamaño de los elementos almacenados. En particular, requiere que todos los elementos de un arreglo tengan el mismo tamaño en bits (por ejemplo, \texttt{int32}, \texttt{float64}, etc.), lo que entra en conflicto con la propuesta de compresión variable y eficiente de \texttt{CompressedVector}, donde los elementos están representados internamente con diferentes esquemas de codificación.

Como consecuencia, al intentar visualizar los datos comprimidos directamente con estas bibliotecas, se hace necesario descomprimirlos previamente. Para ello, la clase \texttt{CompressedVector} ofrece la opción \texttt{get\_decompressed=True}, que entrega una representación expandida compatible con \texttt{numpy}. Sin embargo, este proceso elimina los beneficios de la compresión, incrementando significativamente la memoria alocada y desaprovechando las ventajas espaciales del enfoque.

Resolver esta limitación está fuera del alcance de este trabajo. Sería necesario modificar el código fuente de estas bibliotecas para aceptar iteradores genéricos sin forzar una conversión a \texttt{numpy}, o bien desarrollar un nuevo tipo de dato compatible con la infraestructura de \texttt{numpy}. Ambos caminos implican un esfuerzo considerable y una reestructuración profunda del ecosistema.

\subsubsection{PyGal}

\texttt{PyGal} es una biblioteca orientada a la generación de gráficos SVG embebibles para la Web~\cite{pygal}. Aunque ofrece funcionalidades básicas como gráficos de líneas y dispersión, se destaca por una característica clave: acepta cualquier objeto iterable como fuente de datos, sin necesidad de convertirlo a \texttt{numpy}. Gracias a esto, \texttt{PyGal} es plenamente compatible con \texttt{CompressedVector} y \texttt{CompressedVectorDownsampler}.

No obstante, esta flexibilidad tiene un costo. \texttt{PyGal} presenta limitaciones notables en cuanto a interactividad y personalización. No permite acciones como zoom, selección dinámica de puntos o animaciones avanzadas, lo que restringe su utilidad en contextos donde el análisis visual requiere exploración activa y responsiva.

\subsubsection{Vega-Altair}

\texttt{Vega-Altair} es una biblioteca declarativa para la visualización de datos en Python, basada en la gramática de gráficos de \texttt{Vega-Lite}~\cite{altair}. Su API es coherente, legible y potente, permitiendo crear visualizaciones complejas con poco código.

Una de sus principales ventajas es su compatibilidad con estructuras iterables sin requerir conversión explícita a \texttt{numpy}. Esto la hace adecuada para trabajar directamente con \texttt{CompressedVectorDownsampler}, permitiendo visualizar datos comprimidos y downsampleados sin perder eficiencia espacial.

\texttt{Altair} combina muchas de las fortalezas de bibliotecas como \texttt{plotly} y \texttt{matplotlib}, ofreciendo soporte para múltiples tipos de gráficos, interactividad avanzada y buena integración con el ecosistema de análisis de datos en Python. Por estas razones, ha sido la herramienta principal utilizada en los experimentos de esta memoria y se recomienda como la opción preferente para trabajos futuros.


\subsection{Framework de Experimentación}
\label{benchmarking}
%aqui falta hilar un poco estas ideas...
Benchmarking es el proceso continuo de comparar el desempeño, productos o prácticas de una organización, sistema o componente con referentes externos —como líderes de la industria o soluciones de alto rendimiento— con el objetivo de identificar brechas y oportunidades de mejora. Este proceso no se limita sólo a la recolección de métricas, sino que también implica el análisis crítico y, cuando es pertinente, la adopción o adaptación de ideas y métodos que han demostrado ser eficaces.~\cite{stapenhurst2009benchmarking}

En el contexto de la Memoria de Título, el benchmarking se refiere a los experimentos definidos en el anexo\ref{sec:experimentos}, que evalúan distintos aspectos de la implementación propuesta: espacio, memoria, tiempo de renderización y construcción, entre otros.

Para ese objetivo, se desarrolló un framework de experimentación que se detalla en las subsecciones a continuación.

\subsubsection{Sacred}

Para llevar a cabo las comparaciones se utilizó la biblioteca \texttt{Sacred}, una herramienta de Python que permite configurar, organizar, registrar y reproducir experimentos computacionales~\cite{greff2017sacred}. Está diseñada para introducir una sobrecarga mínima, fomentando la modularidad y flexibilidad en su implementación.

Esta biblioteca permite ejecutar experimentos de forma estructurada, manteniendo control sobre los parámetros involucrados. Se definió una configuración base con parámetros comunes, pero cada experimento puede especificar valores propios que reemplazan a los generales cuando corresponde.

Por ejemplo, se estableció globalmente que cada experimento debe ejecutarse 100 veces, con el fin de obtener un promedio representativo. Sin embargo, en ciertos casos —como en las pruebas de espacio, donde los resultados son estables y no requieren repetición— este valor se reemplaza localmente por una sola ejecución. Así, mientras los ensayos de tiempo usan múltiples iteraciones por defecto, los de uso de espacio en disco se realizan solo una vez.

\subsubsection{Input Handler}

\textit{InputHandler} es una clase auxiliar que permite cargar datos desde archivos CSV y convertirlos en diferentes formatos, ya sea sin compresión, con compresión usando \textit{CompressedVector}, o bien aplicando técnicas de reducción de puntos. Su diseño parametrizable permite ajustar el formato de los datos para facilitar su uso en pruebas de benchmarking.

\vspace{0.5em}
\textbf{Configuración:} mediante el método \textit{set\_width()}, se puede definir el ancho de bits para la parte entera de los datos en los ejes \textit{x} e \textit{y}, lo que incide directamente en la eficiencia del almacenamiento comprimido.

\vspace{0.5em}
\textbf{Extracción de datos:} la función \textit{get\_from\_file()} permite cargar columnas desde archivos CSV, entregando los datos en distintos formatos según el parámetro \textit{option}:
\begin{itemize}
    \item \textit{``default"}: datos crudos en \textit{float}.
    \item \textit{``compressed\_vector"}: representación comprimida con \textit{CompressedVector}.
    \item \textit{``compressed\_vector\_downsampler"}: compresión + reducción usando \textit{CompressedVectorDownsampler}.
    \item \textit{``tsdownsample"}: reducción usando librería externa \textit{tsdownsample}.
\end{itemize}

\vspace{0.5em}
\textbf{Compresión:} el método interno \textit{compress\_vector()} se encarga de transformar una lista de valores en un \textit{CompressedVector}, aplicando las opciones de redondeo, compresión, y configuración de bits especificadas por el usuario.

\vspace{0.5em}
\textbf{Índices seleccionados:} luego de aplicar reducción, es posible recuperar los índices de los puntos seleccionados mediante \textit{get\_x\_indices()} y \textit{get\_y\_indices()}, lo que resulta útil para visualizar los puntos elegidos o analizar su distribución.

\subsubsection{Experiment Runner}

Llamamos \textit{Experiment Runner} a una serie de herramientas definidas en el benchmarking que nos permiten ejecutar cualquier experimento previamente definido. Este módulo gestiona las configuraciones globales, aplica los parámetros correspondientes a cada caso, ejecuta la función de evaluación y almacena los resultados mediante la librería \textit{Sacred} en un archivo \textit{.json}.

Por cada experimento, se registra la siguiente información:

\begin{itemize}
    \item \textit{option}: tipo de representación utilizada (ej. \textit{default}, \textit{compressed\_vector}, etc.).
    \item \textit{file}: nombre base del archivo de entrada.
    \item \textit{n\_size}: cantidad de datos utilizados en la muestra.
    \item \textit{n\_out}: tamaño de la salida en caso de reducción.
    \item \textit{mean}: promedio del valor medido en las iteraciones.
    \item \textit{stdev}: desviación estándar del valor medido.
    \item \textit{min} y \textit{max}: valores mínimo y máximo observados.
    \item \textit{all\_differences}: lista completa con los resultados individuales por iteración.
    \item \textit{iterations}: cantidad de veces que se repitió el experimento.
    \item \textit{measurement\_unit}: unidad de medida correspondiente (por ejemplo, milisegundos o bytes).
\end{itemize}

Estos datos permiten evaluar de forma cuantitativa el comportamiento de diferentes estrategias de compresión y visualización frente a distintos volúmenes de entrada.

Además de los resultados cuantitativos de cada experimento, el archivo \textit{.json} generado por \textit{Sacred} incluye información complementaria útil para trazabilidad y reproducibilidad. Por ejemplo, se registra el nombre del experimento, la ruta base de ejecución, las versiones exactas de las dependencias utilizadas, los archivos fuente involucrados, y metadatos del sistema donde se ejecutó (CPU, sistema operativo, versión de Python, etc.). También se incluyen los commits y estado del repositorio \textit{git}, permitiendo asociar los resultados directamente con una versión específica del código. Esta información permite contextualizar los resultados y asegurar que los experimentos puedan ser replicados en el futuro con fidelidad.

\subsubsection{Plantilla De Experimentos}

Dentro del repositorio, el archivo \textit{template.py} actúa como base para definir cualquier experimento compatible con el framework de benchmarking. Este archivo contiene una estructura mínima y parametrizable que permite registrar, configurar y ejecutar un experimento completo con la ayuda de \textit{Sacred}.

La estructura del experimento tiene tres componentes clave:

\begin{itemize}
    \item \textbf{Título del experimento:} definido mediante \textit{exp\_name}, permite organizar los resultados en carpetas separadas y legibles.
    \item \textbf{Configuración:} a través del decorador \textit{@exp.config} se definen los \textit{casos de prueba} (por ejemplo, vectores sin comprimir, comprimidos, o reducidos), así como el rango de tamaños de entrada y otros parámetros globales.
    \item \textbf{Función principal:} bajo el decorador \textit{@exp.automain}, se implementa la función que será ejecutada con los parámetros definidos. Esta instancia recibe una función \textit{experiment\_fn}, que representa la lógica específica del experimento (por ejemplo, medir tiempo de ejecución), y se conecta con el \textit{Experiment Runner} para la ejecución repetida, registro de métricas y almacenamiento de resultados.
\end{itemize}

Dentro de \textit{experiment\_fn(x, y, option)}, el usuario define lo que desea medir utilizando los datos entregados por el \textit{InputHandler}. Por ejemplo, se puede medir el tiempo de ejecución de un proceso sobre los datos \textit{x} e \textit{y}, o bien calcular la precisión de una reconstrucción. Este valor es luego devuelto al \textit{Experiment Runner}, que lo almacena junto con las configuraciones usadas.

Gracias a esta plantilla, definir nuevos experimentos se vuelve un proceso sencillo, flexible y reutilizable.